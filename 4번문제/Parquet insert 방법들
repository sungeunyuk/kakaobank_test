
일 1억건 mysql data 를 HDFS - parquet 파일로 저장 

1. java - mysql 테이블 read
  ㄱ. avroParquet - hdfs insert 
  ㄴ. hadoop - hive(parquet-table) insert query 로 파싱하여 쿼리 수행 (1건씩 insert) 
  ㄷ. avroParquet - java 상에서 hdfs 명령어 수행 (hadoop fs -put /{local_dir}/test_file.parquet
  ㄹ. sqoop 이용
  ㅁ. nifi - mysql CDC 프로세스 활용하여 하둡테이블로 바로 insert 
